{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Exploration considering Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Theoretical view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-1. Posterior Sampling for Reinforcement Learning (PSRL)\n",
    "\n",
    "Assume the reward $\\mu$ and the transition $P$ is stochastic. For each episode, one sample an MDP from the posterior distribution, conditioned on the history $\\mathcal{F}_{t}$ that is generated up to episode $t$. Then, the algorithm computes the optimal policy given the sampled MDP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. Uncertainty Bellman Equation (UBE)\n",
    "\n",
    "Assume the posterior distributions of $\\mu$ and $P$ can be derived. Then, $\\textbf{Uncertainty Bellman Equation}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "u_{sa}^{h}\n",
    "=\\nu_{sa}^{h}+\\sum_{s'a'}\\pi_{s'a'}^{h}\\mathbb{E}\\left( P_{s'sa}^{h} | \\mathcal{F}_{t} \\right)u_{s'a'}^{h+1}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\nu_{sa}^{h}$ is the local uncertainty at $(s,a)$, which is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\nu_{sa}^{h}=\\mathrm{Var}\\left(\\hat{\\mu}_{sa}^{h} | \\mathcal{F}_{t} \\right)+Q_{\\mathrm{max}}^{2}\\sum_{s'}\\frac{\\mathrm{Var}\\left(\\hat{P}_{s'sa}^{h} | \\mathcal{F}_{t} \\right)}{\\mathbb{E}\\left( \\hat{P}_{s'sa}^{h} | \\mathcal{F}_{t} \\right)}\n",
    "\\end{equation}\n",
    "\n",
    "Given UBE, one can approximate the posterior distribution of Q value as\n",
    "\n",
    "\\begin{gather*}\n",
    "Q_{sa}^{h}\\sim\\mathcal{N}\\left( \\bar{Q}_{sa}^{h}, \\mathrm{diag}(u_{sa}^{h}) \\right) \\\\\n",
    "\\bar{Q}_{sa}^{h}=\\mathbb{E}\\left( \\hat{\\mu}_{sa}^{h} | \\mathcal{F}_{t} \\right)+\\sum_{s',a'}\\pi_{s'a'}^{h}\\mathbb{E}\\left( \\hat{P}_{s'sa}^{h} | \\mathcal{F}_{t} \\right)\\bar{Q}_{s'a'}^{h+1}\n",
    "\\end{gather*}\n",
    "\n",
    "and use it to perform Thompson sampling(=posterior sampling) as an exploration strategy.\n",
    "\n",
    "\\begin{gather*}\n",
    "a=\\mathrm{argmax}_{b}\\left( \\bar{Q}_{sb}^{h}+\\epsilon_{b}\\cdot\\left( u_{sb}^{h} \\right)^{0.5} \\right) \\\\\n",
    "\\epsilon_{b}\\sim\\mathcal{N}(0,1)\n",
    "\\end{gather*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model-free Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1. Bootstrapped DQN\n",
    "\n",
    "Bootstrapped DQN uses multi-head Q function. For each episode, the agent sample a head randomly and follow greedy policy w.r.t. the sampled head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. UCB Exploration via Q-Ensembles\n",
    "\n",
    "Q-ensemble agent also uses multi-head Q function. However, Q-ensemble agent does not sample a head. Instead, the agent uses whole the heads to calculate the value function. \n",
    "\n",
    "In addition, it adapts an exploration strategy which is based on 'optimism in the face of uncertainty (OFU)'. By adding a empirical standard deviation to the average of Q ensemble heads, and choosing the action whose optimistic Q estimate is highest, the agent deals with the problem of exploration-exploitation dilemma."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3 Bayes-by-Backprop Q-network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model-based Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. Probabilistic Inference for Learning Control (PILCO)\n",
    "\n",
    "Assume deterministic transition model with the Gaussian noise, and the objective is to find a policy $\\pi$ that minimize the expected total cost (how far away a state is from the destination)\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Transition: }s_{t+1} &= f(s_{t},a_{t}) \\\\\n",
    "\\text{Target: }\\Delta_{t} &= s_{t+1} - s_{t} + \\varepsilon \\\\\n",
    "\\end{align*}\n",
    "\\begin{equation}\n",
    "\\varepsilon \\sim \\mathcal{N}(0,\\mathrm{diag}(\\sigma_{\\varepsilon}))\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{gather*}\n",
    "\\mathrm{Obj: }\\min_{\\theta}\\mathcal{J}^{\\pi}(\\theta)=\\sum_{t=0}^{T}\\mathbb{E}_{s_{t}}[ c(s_{t}) ] \\\\\n",
    "c(s_{t}) = 1-\\exp\\left( -\\frac{1}{2\\sigma_{c}^{2}}\\|s_{t}-s_{\\mathrm{target}}\\|^{2} \\right)\n",
    "\\end{gather*}\n",
    "\n",
    "In order to consider model uncertainty, which is essential for the prediction of transitions to unknown states, probabilistic model is proposed. In PILCO, Gaussian Process is adapted as a dynamics model.\n",
    "\n",
    "- One-step prediction model\n",
    "\n",
    "\\begin{align*}\n",
    "p(s_{t+1} | s_{t},a_{t})\n",
    "&= \\mathcal{N}\\left( s_{t+1} | \\mu_{t+1}, \\Sigma_{t+1} \\right) \\\\\n",
    "\\mu_{t+1} &=  s_{t} + \\mathbb{E}_{f}[ \\Delta_{t} ] \\\\\n",
    "\\Sigma_{t+1} &= \\mathrm{Var}_{f}[ \\Delta_{t} ]\n",
    "\\end{align*}\n",
    "\n",
    "- GP dyamics model\n",
    "\n",
    "\\begin{gather*}\n",
    "\\mathbf{x}:=(s,a) \\\\\n",
    "k(\\mathbf{x}_{t},\\mathbf{x}_{t'}) = \\alpha^{2}\\exp \\left(-\\frac{1}{2}||\\mathbf{x}_{t}-\\mathbf{x}_{t'}||_{\\mathbf{\\Lambda}}\\right)\n",
    "\\end{gather*}\n",
    "\n",
    "\\begin{equation}\n",
    "f \\sim GP(m_{f}, k(\\cdot,\\cdot))\n",
    "\\end{equation}\n",
    "\n",
    "Evaluating $\\mathcal{J}^{\\pi}$ requires the state distributions $p(s_{1}), ... , p(s_{T})$, which would be approximated by using one-step prediction and GP dynamics model. Then, policy evaluation $\\mathcal{J}^{\\pi}(\\theta)$ and policy gradient $\\mathrm{d}\\mathcal{J}^{\\pi}(\\theta)/\\mathrm{d}\\theta$ can be computed analytically.\n",
    "\n",
    "However, it does not scale well w.r.t. bthe number of trials and the dimension of state and action. Also, it cannot consider the temporal correlation of uncertainty in successive steps. DeepPILCO tried to handle these limitations by using Bayesian deep dynamics model and particle method.\n",
    "\n",
    "- One-step prediction model of DeepPILCO\n",
    "\n",
    "\\begin{equation}\n",
    "p(s_{t+1} | s_{t},a_{t}) = \\mathrm{MCDropout}(s_{t}, a_{t};\\mathbf{w}, p)\n",
    "\\end{equation}\n",
    "\n",
    "- Bayesian deep dynamics model\n",
    "\n",
    "\\begin{gather*}\n",
    "\\mathbf{w} \\sim p(\\mathbf{w} | \\mathcal{D}) \\\\\n",
    "f_{\\mathbf{w}} = \\mathrm{BayesRNN}(\\mathbf{w})\n",
    "\\end{gather*}\n",
    "\n",
    "- Input distribution estimation\n",
    "\n",
    "\\begin{align*}\n",
    "s_{t}^{(k)} &\\sim p(s_{t}),\\; a_{t}^{(k)} \\sim \\pi (a_{t} | s_{t}^{(k)}) \\\\\n",
    "s_{t+1}^{(k)} &= f_{\\mathbf{w}}(s_{t}^{(k)},a_{t}^{(k)}) \\\\\n",
    "\\hat{\\mu}_{t+1} &= \\frac{1}{K}\\sum_{k=1}^{K}s_{t+1}^{(k)} \\\\\n",
    "\\hat{\\sigma}_{t+1}^{2} &= \\frac{1}{K}\\sum_{k=1}^{K}\\left( s_{t+1}^{(k)} - \\mu_{t+1} \\right)^{2} \\\\\n",
    "p(s_{t+1}) &\\approx \\mathcal{N}(\\hat{\\mu}_{t+1}, \\hat{\\sigma}_{t+1}^{2})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. Probabilistic Ensembles with Trajectory Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-3. Model-Ensemble TRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
